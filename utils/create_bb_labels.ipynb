{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a82c1e3",
   "metadata": {},
   "source": [
    "# Label ASL dataset with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jovyan/yolo')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50350fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"datasets/American-Sign-Language-Letters-1/\"\n",
    "\n",
    "os.listdir('datasets/American-Sign-Language-Letters-1/')\n",
    "label = 'datasets/American-Sign-Language-Letters-1/A0_jpg.rf.7a3779f9166b9d8f3c6e91cc6ff41edb.txt'\n",
    "image = 'datasets/American-Sign-Language-Letters-1/A0_jpg.rf.7a3779f9166b9d8f3c6e91cc6ff41edb.jpg'\n",
    "\n",
    "with open(label, 'r') as f:\n",
    "    line = [float(x) for x in f.readlines()[0].strip().split()][1:]\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc931bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def plot_image_with_bbox(image_path, bbox_xywhn):\n",
    "    \"\"\"\n",
    "    Reads an image and plots it with a bounding box in normalized xywh format.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file.\n",
    "        bbox_xywhn (np.ndarray or list or tensor): Normalized bounding box (x_center, y_center, width, height).\n",
    "    \"\"\"\n",
    "    # Read and convert image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    \n",
    "    # Ensure bbox is a flat numpy array\n",
    "    bbox = np.array(bbox_xywhn).flatten()\n",
    "    x_center, y_center, w, h = bbox\n",
    "\n",
    "    # Convert normalized to absolute coordinates\n",
    "    abs_x = (x_center - w / 2) * img_w\n",
    "    abs_y = (y_center - h / 2) * img_h\n",
    "    abs_w = w * img_w\n",
    "    abs_h = h * img_h\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    rect = patches.Rectangle((abs_x, abs_y), abs_w, abs_h,\n",
    "                             linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_with_bbox(image, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392fdad",
   "metadata": {},
   "source": [
    "# Create ASL Dataset with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from Kaggle\n",
    "#!wget https://www.kaggle.com/api/v1/datasets/download/debashishsau/aslamerican-sign-language-aplhabet-dataset -O datasets/aslamerican-sign-language-aplhabet-dataset.zip\n",
    "#!unzip datasets/aslamerican-sign-language-aplhabet-dataset.zip -d datasets/aslamerican-sign-language-aplhabet-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# # path to trained key point estimation model \n",
    "# model_path = \"/home/jovyan/yolo/runs/pose/train/weights/best.pt\"\n",
    "# # Load the model\n",
    "# model = YOLO(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14318e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import json\n",
    "\n",
    "\n",
    "# class_names = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \n",
    "#                 \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\",\n",
    "#                 \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "# output_folder_images = \"/home/jovyan/yolo/datasets/ASL_test/train/images\"\n",
    "# output_folder_labels = \"/home/jovyan/yolo/datasets/ASL_test/train/labels\"\n",
    "\n",
    "# os.makedirs(output_folder_images, exist_ok=True)\n",
    "# os.makedirs(output_folder_labels, exist_ok=True)\n",
    "\n",
    "# class_states = {}\n",
    "# for class_index, class_name in enumerate(class_names): \n",
    "#     image_folder = f\"/home/jovyan/yolo/datasets/ASL_Alphabet_Dataset/asl_alphabet_train/{class_name}\"\n",
    "\n",
    "#     # Efficient inference with streaming\n",
    "#     results = model.predict(source=image_folder, stream=True)\n",
    "\n",
    "#     states = {}\n",
    "#     for i, result in enumerate(results):\n",
    "\n",
    "#         image_name = os.path.basename(result.path)\n",
    "#         new_image_name = f\"{class_name}_{image_name}\"\n",
    "\n",
    "#         if len(result.boxes) == 0:\n",
    "#             print(f\"No bounding boxes detected in image: {image_name}\")\n",
    "#             states[\"no_bbox\"] = states.get(\"no_bbox\", 0) + 1\n",
    "#             continue\n",
    "#         elif len(result.boxes) > 1:\n",
    "#             print(f\"Multiple bounding boxes detected in image: {image_name}\")\n",
    "#             states[\"multiple_bbox\"] = states.get(\"multiple_bbox\", 0) + 1\n",
    "#             continue\n",
    "#         elif result.boxes[0].conf[0] < 0.5:\n",
    "#             print(f\"Low confidence in image: {image_name}\")\n",
    "#             states[\"low_confidence\"] = states.get(\"low_confidence\", 0) + 1\n",
    "#             continue\n",
    "        \n",
    "#         # save original image at output_folder_images\n",
    "#         image_path = os.path.join(image_folder, image_name)\n",
    "#         new_image_path = os.path.join(output_folder_images, new_image_name)\n",
    "#         cv2.imwrite(new_image_path, cv2.imread(image_path))\n",
    "\n",
    "#         # save bounding boxses as txt file like class_index x_center y_center width height in the output_folder_labels\n",
    "#         label_path = os.path.join(output_folder_labels, new_image_name.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "#         #result.boxes[0].xywhn returns tensor([[0.5577, 0.3010, 0.4520, 0.5482]], device='cuda:0') \n",
    "#         x, y, w, h = result.boxes[0].xywhn[0].cpu().numpy()\n",
    "\n",
    "#         with open(label_path, \"w\") as f:\n",
    "#             f.write(f\"{class_index} {x} {y} {w} {h}\")\n",
    "\n",
    "#         class_states[class_name] = states\n",
    "\n",
    "# # save the class states to a json file\n",
    "# with open(\"class_states.json\", \"w\") as f:\n",
    "#     json.dump(class_states, f, indent=4)    \n",
    "\n",
    "# print(class_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21facac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# path to trained key point estimation model \n",
    "model_path = \"/home/jovyan/yolo/runs/pose/train/weights/best.pt\"\n",
    "# Load the model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "class_names = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing A: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8458/8458 [00:10<00:00, 831.14it/s] \n",
      "Processing B: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8309/8309 [00:12<00:00, 672.56it/s]\n",
      "Processing C: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8146/8146 [00:09<00:00, 887.94it/s] \n",
      "Processing D: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7629/7629 [00:09<00:00, 824.27it/s]\n",
      "Processing E: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7744/7744 [00:10<00:00, 765.14it/s] \n",
      "Processing F: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8031/8031 [00:11<00:00, 676.83it/s]\n",
      "Processing G: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7844/7844 [00:09<00:00, 793.32it/s]\n",
      "Processing H: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7906/7906 [00:09<00:00, 852.89it/s]\n",
      "Processing I: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7953/7953 [00:09<00:00, 808.25it/s] \n",
      "Processing J: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7503/7503 [00:06<00:00, 1073.27it/s]\n",
      "Processing K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7876/7876 [00:10<00:00, 749.78it/s]]\n",
      "Processing L: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7939/7939 [00:11<00:00, 708.54it/s]]\n",
      "Processing M: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7900/7900 [00:06<00:00, 1134.92it/s]\n",
      "Processing N: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7932/7932 [00:06<00:00, 1276.69it/s]\n",
      "Processing O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8140/8140 [00:07<00:00, 1065.85it/s]\n",
      "Processing P: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7601/7601 [00:07<00:00, 1012.20it/s]\n",
      "Processing Q: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7954/7954 [00:08<00:00, 897.82it/s]]\n",
      "Processing R: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8021/8021 [00:08<00:00, 909.17it/s] \n",
      "Processing S: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8109/8109 [00:08<00:00, 934.59it/s] \n",
      "Processing T: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8054/8054 [00:10<00:00, 786.37it/s] \n",
      "Processing U: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8023/8023 [00:10<00:00, 773.46it/s]]\n",
      "Processing V: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7597/7597 [00:11<00:00, 680.79it/s]]\n",
      "Processing W: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7787/7787 [00:11<00:00, 688.49it/s]]\n",
      "Processing X: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8093/8093 [00:08<00:00, 911.35it/s] \n",
      "Processing Y: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8178/8178 [00:09<00:00, 857.57it/s] \n",
      "Processing Z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7410/7410 [00:06<00:00, 1195.46it/s]\n",
      "Processing classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [16:08<00:00, 37.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'multiple_bbox': 42, 'no_bbox': 1739, 'low_confidence': 609}, 'B': {'multiple_bbox': 42, 'low_confidence': 224, 'no_bbox': 387}, 'C': {'low_confidence': 497, 'multiple_bbox': 130, 'no_bbox': 1613}, 'D': {'multiple_bbox': 153, 'no_bbox': 1055, 'low_confidence': 640}, 'E': {'multiple_bbox': 49, 'no_bbox': 1014, 'low_confidence': 490}, 'F': {'multiple_bbox': 39, 'no_bbox': 216, 'low_confidence': 245}, 'G': {'multiple_bbox': 369, 'low_confidence': 342, 'no_bbox': 702}, 'H': {'multiple_bbox': 318, 'no_bbox': 1016, 'low_confidence': 499}, 'I': {'no_bbox': 1194, 'low_confidence': 624, 'multiple_bbox': 181}, 'J': {'no_bbox': 1138, 'low_confidence': 403, 'multiple_bbox': 30}, 'K': {'multiple_bbox': 166, 'no_bbox': 369, 'low_confidence': 204}, 'L': {'multiple_bbox': 135, 'no_bbox': 181, 'low_confidence': 112}, 'M': {'multiple_bbox': 211, 'low_confidence': 1116, 'no_bbox': 2109}, 'N': {'multiple_bbox': 141, 'no_bbox': 2723, 'low_confidence': 1221}, 'O': {'no_bbox': 2357, 'low_confidence': 1126, 'multiple_bbox': 56}, 'P': {'low_confidence': 777, 'no_bbox': 1675, 'multiple_bbox': 150}, 'Q': {'low_confidence': 652, 'no_bbox': 1188, 'multiple_bbox': 119}, 'R': {'no_bbox': 597, 'low_confidence': 513, 'multiple_bbox': 1216}, 'S': {'no_bbox': 1692, 'low_confidence': 781, 'multiple_bbox': 54}, 'T': {'low_confidence': 425, 'no_bbox': 621, 'multiple_bbox': 51}, 'U': {'multiple_bbox': 65, 'low_confidence': 352, 'no_bbox': 699}, 'V': {'no_bbox': 254, 'low_confidence': 135, 'multiple_bbox': 48}, 'W': {'no_bbox': 275, 'low_confidence': 160, 'multiple_bbox': 16}, 'X': {'low_confidence': 577, 'no_bbox': 1756, 'multiple_bbox': 23}, 'Y': {'multiple_bbox': 92, 'low_confidence': 578, 'no_bbox': 1258}, 'Z': {'no_bbox': 1444, 'multiple_bbox': 134, 'low_confidence': 674}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "\n",
    "output_folder_images = \"./data/ASL/data/images\"\n",
    "output_folder_labels = \"./data/ASL/data/labels\"\n",
    "\n",
    "os.makedirs(output_folder_images, exist_ok=True)\n",
    "os.makedirs(output_folder_labels, exist_ok=True)\n",
    "\n",
    "# if class_states exist, load it \n",
    "if os.path.exists(\"class_states.json\"):\n",
    "    with open(\"class_states.json\", \"r\") as f:\n",
    "        class_states = json.load(f)\n",
    "else:\n",
    "    # if class_states does not exist, create it\n",
    "    class_states = {}\n",
    "\n",
    "def process_result(result, class_index, class_name, states, image_folder):\n",
    "    image_name = os.path.basename(result.path)\n",
    "    new_image_name = f\"{class_name}_{image_name}\"\n",
    "\n",
    "    if len(result.boxes) == 0:\n",
    "        states[\"no_bbox\"] = states.get(\"no_bbox\", 0) + 1\n",
    "        return\n",
    "\n",
    "    if len(result.boxes) > 1:\n",
    "        states[\"multiple_bbox\"] = states.get(\"multiple_bbox\", 0) + 1\n",
    "        return\n",
    "\n",
    "    if result.boxes[0].conf[0] < 0.5:\n",
    "        states[\"low_confidence\"] = states.get(\"low_confidence\", 0) + 1\n",
    "        return\n",
    "\n",
    "    # Read and save image\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is not None:\n",
    "        cv2.imwrite(os.path.join(output_folder_images, new_image_name), image)\n",
    "\n",
    "    # Save label\n",
    "    x, y, w, h = result.boxes[0].xywhn[0].cpu().numpy()\n",
    "    label_path = os.path.join(output_folder_labels, new_image_name.replace(\".jpg\", \".txt\"))\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.write(f\"{class_index} {x} {y} {w} {h}\")\n",
    "\n",
    "for class_index, class_name in tqdm(enumerate(class_names), desc=\"Processing classes\", total=len(class_names)):\n",
    "\n",
    "    image_folder = f\"./data/ASL_Alphabet_Dataset/asl_alphabet_train/{class_name}\"\n",
    "    \n",
    "    # Non-streaming, batch prediction\n",
    "    results = model.predict(source=image_folder, stream=False, batch=512, verbose=False)\n",
    "    \n",
    "    states = {}\n",
    "    for result in tqdm(results, desc=f\"Processing {class_name}\", total=len(results)):\n",
    "        process_result(result, class_index, class_name, states, image_folder)\n",
    "\n",
    "    class_states[class_name] = states\n",
    "\n",
    "# Save class states\n",
    "with open(\"class_states.json\", \"w\") as f:\n",
    "    json.dump(class_states, f, indent=4)\n",
    "\n",
    "print(class_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87c383",
   "metadata": {},
   "source": [
    "# Analyse new Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "base_dir = \"/home/jovyan/yolo/datasets/ASL_test\"\n",
    "input_images_dir = os.path.join(base_dir, \"data/images\")\n",
    "input_labels_dir = os.path.join(base_dir, \"data/labels\")\n",
    "\n",
    "class_groups = defaultdict(list)\n",
    "for fname in os.listdir(input_images_dir):\n",
    "    if fname.endswith(\".jpg\") or fname.endswith(\".jpeg\"):\n",
    "        class_prefix = fname.split(\"_\")[0]  # Assumes format like A_image1.jpg\n",
    "        class_groups[class_prefix].append(fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036b2258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class D has 493 images.\n",
      "Class U has 541 images.\n",
      "Class W has 678 images.\n",
      "Class Y has 386 images.\n",
      "Class N has 197 images.\n",
      "Class P has 454 images.\n",
      "Class B has 654 images.\n",
      "Class C has 108 images.\n",
      "Class Z has 291 images.\n",
      "Class Q has 503 images.\n",
      "Class I has 320 images.\n",
      "Class K has 584 images.\n",
      "Class O has 38 images.\n",
      "Class E has 269 images.\n",
      "Class L has 620 images.\n",
      "Class A has 405 images.\n",
      "Class F has 696 images.\n",
      "Class V has 622 images.\n",
      "Class H has 542 images.\n",
      "Class G has 372 images.\n",
      "Class R has 397 images.\n",
      "Class X has 165 images.\n",
      "Class T has 215 images.\n",
      "Class S has 220 images.\n",
      "Class J has 161 images.\n",
      "Class M has 113 images.\n"
     ]
    }
   ],
   "source": [
    "for class_prefix, images in class_groups.items():\n",
    "    print(f\"Class {class_prefix} has {len(images)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaa839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/home/jovyan/yolo/datasets/ASL_test\"\n",
    "input_images_dir = os.path.join(base_dir, \"data/images\")\n",
    "input_labels_dir = os.path.join(base_dir, \"data/labels\")\n",
    "\n",
    "output_dirs = {\n",
    "    \"train\": {\n",
    "        \"images\": os.path.join(base_dir, \"train/images\"),\n",
    "        \"labels\": os.path.join(base_dir, \"train/labels\")\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"images\": os.path.join(base_dir, \"val/images\"),\n",
    "        \"labels\": os.path.join(base_dir, \"val/labels\")\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"images\": os.path.join(base_dir, \"test/images\"),\n",
    "        \"labels\": os.path.join(base_dir, \"test/labels\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for split in output_dirs:\n",
    "    os.makedirs(output_dirs[split][\"images\"], exist_ok=True)\n",
    "    os.makedirs(output_dirs[split][\"labels\"], exist_ok=True)\n",
    "\n",
    "# Define split ratios\n",
    "split_ratios = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n",
    "\n",
    "# Group images by class prefix (e.g., A_, B_, ...)\n",
    "class_groups = defaultdict(list)\n",
    "for fname in os.listdir(input_images_dir):\n",
    "    if fname.endswith(\".jpg\") or fname.endswith(\".jpeg\"):\n",
    "        class_prefix = fname.split(\"_\")[0]  # Assumes format like A_image1.jpg\n",
    "        class_groups[class_prefix].append(fname)\n",
    "\n",
    "# Process each class group\n",
    "for class_name, image_list in class_groups.items():\n",
    "    random.shuffle(image_list)\n",
    "    total = len(image_list)\n",
    "\n",
    "    train_end = int(split_ratios[\"train\"] * total)\n",
    "    val_end = train_end + int(split_ratios[\"val\"] * total)\n",
    "\n",
    "    split_map = {\n",
    "        \"train\": image_list[:train_end],\n",
    "        \"val\": image_list[train_end:val_end],\n",
    "        \"test\": image_list[val_end:]\n",
    "    }\n",
    "\n",
    "    for split, images in split_map.items():\n",
    "        for img_name in images:\n",
    "            # Move image\n",
    "            src_img = os.path.join(input_images_dir, img_name)\n",
    "            dst_img = os.path.join(output_dirs[split][\"images\"], img_name)\n",
    "            shutil.move(src_img, dst_img)\n",
    "\n",
    "            # Move label\n",
    "            label_name = img_name.replace(\".jpg\", \".txt\")\n",
    "            src_label = os.path.join(input_labels_dir, label_name)\n",
    "            dst_label = os.path.join(output_dirs[split][\"labels\"], label_name)\n",
    "            if os.path.exists(src_label):\n",
    "                shutil.move(src_label, dst_label)\n",
    "\n",
    "print(\"Dataset split completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f35990",
   "metadata": {},
   "source": [
    "# Train YOLOv11 on the novel dataset to predict the letter and the bb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b910a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 57.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.134 ðŸš€ Python-3.10.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA A100-PCIE-40GB, 40442MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=256, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/jovyan/yolo/datasets/ASL_test/data.yaml, degrees=0.0, deterministic=True, device=4, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=26\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    435742  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,594,910 parameters, 2,594,894 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 449.4Â±210.3 MB/s, size: 13.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jovyan/yolo/datasets/ASL_test/train/labels... 119040 images, 8025 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127065/127065 [01:47<00:00, 1177.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jovyan/yolo/datasets/ASL_test/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 425.6Â±195.8 MB/s, size: 12.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/yolo/datasets/ASL_test/val/labels... 14871 images, 994 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15865/15865 [00:15<00:00, 1007.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jovyan/yolo/datasets/ASL_test/val/labels.cache\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.002), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      37.4G      1.505      4.632      1.805        523        640:   2%|â–         | 11/497 [00:55<30:44,  3.80s/it] "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/home/jovyan/yolo/datasets/ASL_test/data.yaml\", epochs=50, batch=256, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cec48d",
   "metadata": {},
   "source": [
    "# Test on Test-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145d5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.134 ðŸš€ Python-3.10.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA A100-PCIE-40GB, 40442MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,587,222 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 517.1Â±81.7 MB/s, size: 13.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/yolo/datasets/ASL_test/test/labels... 14904 images, 1025 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15929/15929 [00:13<00:00, 1224.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jovyan/yolo/datasets/ASL_test/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 996/996 [00:58<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      15929      14904      0.998      0.997      0.995      0.964\n",
      "                     A        567        567      0.998      0.998      0.995      0.974\n",
      "                     B        701        701          1          1      0.995      0.981\n",
      "                     C        581        581      0.992       0.99      0.994      0.957\n",
      "                     D        530        530      0.996      0.995      0.995      0.954\n",
      "                     E        593        593      0.999      0.997      0.995      0.978\n",
      "                     F        684        684      0.997      0.997      0.995      0.979\n",
      "                     G        607        607      0.998      0.997      0.994       0.93\n",
      "                     H        554        554      0.997      0.996      0.995      0.962\n",
      "                     I        564        564      0.993      0.996      0.995      0.969\n",
      "                     J        578        578          1      0.995      0.995      0.966\n",
      "                     K        656        656      0.997      0.998      0.995      0.987\n",
      "                     L        690        690      0.998          1      0.995      0.976\n",
      "                     M        436        436          1      0.999      0.995       0.95\n",
      "                     N        365        365      0.999      0.995      0.995      0.942\n",
      "                     O        457        457      0.993      0.996      0.995      0.938\n",
      "                     P        455        455      0.999          1      0.995      0.933\n",
      "                     Q        550        550          1          1      0.995      0.968\n",
      "                     R        531        531      0.996      0.995      0.995      0.976\n",
      "                     S        537        537      0.998      0.981      0.995      0.951\n",
      "                     T        675        675      0.998      0.999      0.995      0.976\n",
      "                     U        638        638          1          1      0.995      0.981\n",
      "                     V        655        655          1      0.998      0.995      0.982\n",
      "                     W        667        667          1          1      0.995      0.979\n",
      "                     X        558        558      0.999          1      0.995      0.965\n",
      "                     Y        587        587          1          1      0.995      0.976\n",
      "                     Z        488        488      0.994      0.994      0.995      0.946\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set \n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"/home/jovyan/yolo/runs/detect_asl/train/weights/best.pt\"\n",
    "model = YOLO(model_path)  # load a custom model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = model.val(data=\"/home/jovyan/yolo/datasets/ASL_test/data.yaml\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c63e7",
   "metadata": {},
   "source": [
    "# App.py on KI-Server\n",
    "\n",
    "Jupyter Notebook and YOLO train does not work very well due to doing this in Jupyter or IPython, memory leaks are much more common due to how the kernel retains references. Use standalone .py scripts if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870bf796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jovyan/yolo/yolo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/jovyan/yolo/yolo.py\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"/home/jovyan/yolo/runs/detect_asl/train2/weights/last.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "results = model.train(data=\"/home/jovyan/yolo/datasets/ASL_test/data.yaml\", epochs=5, batch=128, imgsz=640, project=\"runs/detect_asl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (GPU 4)",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
